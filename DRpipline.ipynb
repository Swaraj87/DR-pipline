{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c2f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e413af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84072381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_auc_score, cohen_kappa_score)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0e9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        #dataset pathways\n",
    "        self.data_dir = Path('Modular Implementation/aptos2019-blindness-detection')\n",
    "        self.train_csv = self.data_dir/'train.csv'\n",
    "        self.train_image = self.data_dir/'train_images.csv'\n",
    "        self.test_image = self.data_dir/'test_images'\n",
    "\n",
    "        #Model path\n",
    "        self.model_dir = Path('savemodels')\n",
    "        self.model_features = Path('extracted_features')\n",
    "        self.model_results = Path('results')\n",
    "\n",
    "        #Create Directories \n",
    "        for dir_path in [self.model_dir, self.model_features, self.model_results]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 10\n",
    "        self.learning_rate = 1e-4\n",
    "        self.num_classes = 5 #(0-4 severity)\n",
    "        self.img_size = (512, 512) # for highlightling retina images\n",
    "\n",
    "        # Models\n",
    "        self.pretrained_models = {\n",
    "            'resnet50': models.resnet50,\n",
    "            'densenet121': models.densenet121,\n",
    "            'inceptionV3': models.inception_v3\n",
    "        }\n",
    "\n",
    "        # XGBoost parameters\n",
    "        self.xgb_params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 5,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "        self.n_splits = 5\n",
    "        self.random_seed = 42\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        def to_dict(self):\n",
    "            return {k:v for k,v in self.__dict__.items() if not k.startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4844f3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataAugmentation:\n",
    "    \"performing Data augmentation for our images\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_train_transform():\n",
    "\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=30),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_image(image_path):\n",
    "        transfrom = transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return transfrom(image).unsqueeze(0)\n",
    "11        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cab1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAptos(Dataset):\n",
    "    #Customizing the AptosDataset for our use\n",
    "    def __init__(self, dataframe, image_dir, transform = None, is_test = False):\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img_name = self.dataframe.iloc[idx]['id_code']\n",
    "        \n",
    "            # Handle different image extensions\n",
    "            image_paths = [\n",
    "                self.image_dir / f\"{img_name}.png\",\n",
    "                self.image_dir / f\"{img_name}.jpg\",\n",
    "                self.image_dir / f\"{img_name}.jpeg\"\n",
    "            ]\n",
    "\n",
    "            image_path = None\n",
    "            for path in image_paths:\n",
    "                if path.exists():\n",
    "                    image_path = path\n",
    "                    break\n",
    "            \n",
    "            if image_path is None:\n",
    "                raise FileNotFoundError(f'image not found for{img_name}')\n",
    "            \n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.is_test:\n",
    "                return image, img_name\n",
    "            \n",
    "            label = self.dataframe.iloc[idx]['diagnosis']\n",
    "\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRModelManager:\n",
    "    #Manages multiple pre-trained models for diabetic retinopathy\n",
    "\n",
    "    def __init__(self, config, model_name):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        self.device = config.device\n",
    "        self.model = None\n",
    "        self.feature_extractor = None\n",
    "        self._initialize_model_finetune()\n",
    "\n",
    "    def _initialize_model_finetune(self):\n",
    "        \"\"\"Initialize pre-trained model with fine-tuning on last blocks only\"\"\"\n",
    "        \n",
    "        if self.model_name == 'resnet50':\n",
    "            self._initialize_resnet50_finetune()\n",
    "            \n",
    "        elif self.model_name == 'inceptionV3':\n",
    "            self._initialize_inception_v3_finetune()\n",
    "            \n",
    "        elif self.model_name == 'densenet121':\n",
    "            self._initialize_densenet121_finetune()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Model {self.model_name} not supported\")\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Create feature extractor (all layers except the final classifier)\n",
    "        self._feature_extractor()\n",
    "    \n",
    "    def _initialize_resnet50_finetune(self):\n",
    "\n",
    "        self.model = models.resnet50(pretrained = True)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Also unfreeze the BatchNorm layers in the last block\n",
    "        for module in self.model.layer4.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.train()  # Set to training mode\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        num_feature = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_feature, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, self.config.num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        return f'Succefully initaited {self.model_name}'\n",
    "\n",
    "    def _initialize_inception_v3_finetune(self):\n",
    "            \n",
    "        self.model = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # InceptionV3 architecture: Unfreeze from Mixed_7c onward (last few blocks)\n",
    "        # Mixed_6a to Mixed_7c are the later blocks\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'Mixed_7' in name or 'Mixed_6e' in name or 'Mixed_6d' in name: # type: ignore\n",
    "                param.requires_grad = True\n",
    "            if 'bn' in name or 'BatchNorm' in name:  # Unfreeze BatchNorm in unfrozen blocks\n",
    "                if 'Mixed_7' in name or 'Mixed_6' in name:\n",
    "                    param.requires_grad = True\n",
    "    \n",
    "    # Replace the final FC layer\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, self.config.num_classes)\n",
    "        )\n",
    "        \n",
    "        # Set FC layer to trainable\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        return f'Succefully initaited {self.model_name}'\n",
    "\n",
    "    def _initialize_densenet121_finetune(self):\n",
    "        \"\"\"Fine-tune DenseNet121: only train last dense block\"\"\"\n",
    "        self.model = models.densenet121(pretrained=True)\n",
    "        \n",
    "        # FREEZE ALL LAYERS FIRST\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # DenseNet121: Unfreeze only the last dense block (denseblock4)\n",
    "        # and transition layer before it\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'denseblock4' in name or 'norm5' in name:\n",
    "                param.requires_grad = True\n",
    "            if 'transition3' in name:  # The transition before last block\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze BatchNorm layers in the unfrozen blocks\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                if 'denseblock4' in name or 'norm5' in name:\n",
    "                    module.train()\n",
    "                    for param in module.parameters():\n",
    "                        param.requires_grad = True\n",
    "        \n",
    "        # Replace the classifier\n",
    "        num_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, self.config.num_classes)\n",
    "        )\n",
    "        \n",
    "        # Set classifier to trainable\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        return f'Succefully initaited {self.model_name}'\n",
    "\n",
    "    def _feature_extractor(self):\n",
    "\n",
    "        if self.model_name == 'resnet50':\n",
    "            # Original ResNet50 structure:\n",
    "            # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
    "\n",
    "            # We take everything EXCEPT the final FC layer:\n",
    "            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])\n",
    "            # This gives us: [conv1 → layer4 → avgpool]\n",
    "            # Output shape: (batch_size, 2048, 1, 1) after avgpool\n",
    "        \n",
    "        # InceptionV3 has different structure\n",
    "        # We need to add AdaptiveAvgPool2d because Inception's pooling might vary\n",
    "        elif self.model_name == 'inception_v3':\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                *list(self.model.children())[:-1],\n",
    "                nn.AdaptiveAvgPool2d((1,1))\n",
    "            )\n",
    "        # Output shape: (batch_size, 2048, 1, 1)\n",
    "\n",
    "        # DenseNet structure is different: features + classifier\n",
    "        elif self.model_name == 'densenet121':\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                self.model.features,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d((1,1))\n",
    "            )\n",
    "        # Output shape: (batch_size, 1024, 1, 1)\n",
    "\n",
    "        #Before Extraction of(CNN Output):\n",
    "        #For a batch of 16 images: Shape: (16, 2048, 7, 7)\n",
    "        # 2048 channels, 7x7 spatial grid\n",
    "\n",
    "        #After AdaptiveAvgPool2d((1,1)):\n",
    "        #Shape: (16, 2048, 1, 1)  # Each channel averaged to single value\n",
    "\n",
    "        #Shape: (16, 2048, 1, 1)  # Each channel averaged to single value\n",
    "        #Shape: (16, 2048)  # 2048-dimensional feature vector per image\n",
    "\n",
    "        self.feature_extractor.to(self.device)\n",
    "\n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print which layers are trainable - useful for debugging\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Trainable parameters for {self.model_name}:\")\n",
    "        print('='*60)\n",
    "\n",
    "        total_parameter = 0\n",
    "        trainable_parameter = 0 \n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            total_parameter += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_parameter += param.numel()\n",
    "                print(f\"✓ TRAINABLE: {name}\")\n",
    "            else:\n",
    "                print(f\"  Frozen: {name}\")\n",
    "        \n",
    "        print(f\"\\nTotal parameters: {total_parameter:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_parameter:,}\")\n",
    "        print(f\"Percentage trainable: {100 * trainable_parameter / total_parameter:.2f}%\")\n",
    "        print('='*60)\n",
    "        \n",
    "        return trainable_parameter, total_parameter\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_feature_extractor(self):\n",
    "        return self.feature_extractor\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"Save the important parameters and model details to use even after the training is done\"\n",
    "\n",
    "        trainable_names = []\n",
    "        for name, params in self.model.named_parameters():\n",
    "            if params.requires_grad:\n",
    "                trainable_names.append(name)\n",
    "\n",
    "                torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'model_name': self.model_name,\n",
    "            'config': self.config.to_dict(),\n",
    "            'trainable_layers': trainable_names,\n",
    "            'feature_extractor_state_dict': self.feature_extractor.state_dict()\n",
    "        }, path)\n",
    "                \n",
    "    def load_model(self, path):\n",
    "        #Loading the saved model\n",
    "        checkpoint = torch.load(path, map_location= self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        #loading the trainable parameter if in the model\n",
    "        if 'trainable_layers' in checkpoint:\n",
    "            # First freeze all\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Then unfreeze saved trainable layers\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in checkpoint['trainable_layers']:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        return self.model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRTrainer:\n",
    "    \"Training engine for our fine tune CNNs\"\n",
    "\n",
    "    def __init__(self, config, model_manager):\n",
    "        self.config = config\n",
    "        self.model_manager = model_manager\n",
    "        self.model = self.model_manager.get_model()\n",
    "        self.device = self.config.device\n",
    "\n",
    "        #Printing the Trainable parameter information\n",
    "        self.model_manager.print_trainable_parameters()\n",
    "\n",
    "        training_params = [p for p in self.model.parameter() if p.requires_grad]\n",
    "\n",
    "        if training_params == 0:\n",
    "            raise ValueError(\"No training parameters found. Check the fine tuning.\")\n",
    "        \n",
    "        print(f\"\\nOptimizing {len(training_params)} parameter groups\")\n",
    "        \n",
    "        # Different learning rates for fine-tuned layers vs new layers\n",
    "        # Higher LR for new layers, lower LR for fine-tuned pretrained layers\n",
    "        \n",
    "        # Group parameters by type\n",
    "        new_layers = []\n",
    "        finetune_layers = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'fc' in name or 'classifier' in name:\n",
    "                    new_layers.append(param)  # New classifier layers\n",
    "                else:\n",
    "                    finetune_layers.append(param)  # Fine-tuned pretrained layers\n",
    "        \n",
    "        # Create parameter groups with different learning rates\n",
    "        param_groups = [\n",
    "            {'params': finetune_layers, 'lr': config.learning_rate * 0.1},\n",
    "            {'params': new_layers, 'lr': config.learning_rate}  \n",
    "        ]\n",
    "        self.optimizer = optim.AdamW(param_groups, lr=config.learning_rate)\n",
    "        \n",
    "        #lr scheduler for countinuouly chaning learning and then restarting with higher after some epochs\n",
    "        self.shecduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        # Loss fucntion with class wieght imbalance\n",
    "        self.criterion = self._get_weighted_loss()\n",
    "\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "\n",
    "        def _get_weighted_loss(self):\n",
    "            #Used to assign more weight to less frequency labels in the dataset to avoid baises\n",
    "            #Calculate the weight of each class by - Total sample / no.of classes * count of item in class i\n",
    "\n",
    "            df = pd.read_csv('aptos2019-blindness-detection/train.csv')\n",
    "            counts = df[\"diagnosis\"].value_counts().sort_index()\n",
    "            class_counts = counts.values\n",
    "            class_weights = torch.tensor(\n",
    "                sum(class_counts) / (len(class_counts) * class_counts),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            #trunsout to be tensor([0.4058, 1.9795, 0.7331, 3.7948, 2.4827])\n",
    "            #Normalize weights\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            class_weights.to(self.device)\n",
    "\n",
    "            return nn.CrossEntropyLoss(weight= class_weights)\n",
    "        \n",
    "        def train_epoch(self, train_loader, epoch):\n",
    "            self.model.train()\n",
    "\n",
    "            #Handling the BatchNorm blocks in fine tunning to make sure they are in traning mode\n",
    "            for name, module in self.model.named_modules():\n",
    "                if isinstance(module, nn.BatchNorm2d) and hasattr(module, 'weight'):\n",
    "                    if module.weight.requires_grad:\n",
    "                        module.train()\n",
    "\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.num_epochs}')\n",
    "            for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                #Handling the InceptionV3 duo outputs during training (output, aux_output)\n",
    "\n",
    "                if self.model_manager.model_name == 'inceptionV3':\n",
    "                    outputs, aux_outputs = self.model(inputs)\n",
    "                    # outputs: Main prediction from final layer\n",
    "                    # aux_outputs: Auxiliary prediction from middle layer\n",
    "\n",
    "                    loss1 = self.criterion(outputs, labels)\n",
    "                    loss2 = self.criterion(aux_outputs, labels)\n",
    "                    loss = loss1 + 0.4 * loss2  # Weighted sum as in original paper\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                \n",
    "                #for faster training we use mix precision training where we use FP16 and Fp32\n",
    "                if self.scaler:  # If we have a GPU that supports mixed precision\n",
    "                    # 1. Scale up the loss (prevents underflow)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    # Loss is multiplied by e.g., 65536 before backward pass\n",
    "                    \n",
    "                    # 2. Unscale gradients before optimizer step\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    # Now gradients are back to normal scale\n",
    "                    \n",
    "                    # 3. Clip gradients (prevent overflow)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        [p for p in self.model.parameters() if p.requires_grad], \n",
    "                        max_norm=1.0\n",
    "                    )\n",
    "                    \n",
    "                    # 4. Optimizer step with scaling\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    \n",
    "                    # 5. Update scale factor for next iteration\n",
    "                    self.scaler.update()\n",
    "\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                    #gradient Clipping \n",
    "                    if not hasattr(self, 'trainable_params'):\n",
    "                        self.trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.trainable_params,\n",
    "                        max_norm=1.0\n",
    "                    )\n",
    "\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                #statistics\n",
    "                #running_loss: Sum of all batch losses in the current epoch\n",
    "                #Example: If 100 batches with losses [0.5, 0.4, 0.3, ...], running_loss = 0.5 + 0.4 + 0.3 + ...\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #getting the prediction outputs where we recive 5 output and only choose max value from each iteration\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "\n",
    "                #Gettting the total correctly predicted labels in each iteration\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                #showing the progress bar to monitor the performance\n",
    "                pbar.set_postfix({\n",
    "                    'loss': running_loss / (batch_idx + 1)\n",
    "                    'acc': 100. * correct / total\n",
    "                })\n",
    "\n",
    "            #Managing the loss per epcoh\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = 100. * correct / total\n",
    "\n",
    "            self.history['train_loss'].append(epoch_loss)\n",
    "            self.history['train_acc'].append(epoch_acc)\n",
    "\n",
    "            return epoch_loss, epoch_acc\n",
    "        \n",
    "        def validate(self, val_loader):\n",
    "            #Performing the validation for our trained model\n",
    "            self.model.eval()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                    #For InceptionV3 in eval mode, no aux output\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    running_loss = loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(val_loader)\n",
    "            epoch_acc = 100* correct/ total\n",
    "\n",
    "            self.history['val_loss'].append(epoch_loss)\n",
    "            self.history['val_acc'].append(epoch_acc)\n",
    "\n",
    "            return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "        \n",
    "        def train(self, train_loader, val_loader, start_epoch=0, best_acc=0):\n",
    "            #Complete Traing loopwith fine tunning included\n",
    "\n",
    "            checkpoint_path = self.config.model_dir / f'{self.model_manager.model_name}_finetune_checkpoint.pth'\n",
    "            best_model_path = self.config.model_dir / f\"{self.model_manager.model_name}_finetune_best.pth\"\n",
    "\n",
    "            print(f\"\\nStarting fine-tunning for {self.model_manager.model_name}\")\n",
    "            print(f\"Checkpoint will be saved to: {checkpoint_path}\")\n",
    "\n",
    "            for epoch in range(start_epoch, self.config.num_epochs):\n",
    "                #Adjust learning rate if using warmup\n",
    "                if epoch < 5: #Warmpup phase\n",
    "                    for param_group in self.optimizer.prarm_groups:\n",
    "                        param_group ['lr'] = self.config.learning_rate * (epoch + 1) / 5\n",
    "\n",
    "                        #Train\n",
    "                        train_loss, train_acc = self.train_epoch(train_loader, epoch)\n",
    "                        \n",
    "                        #Validate\n",
    "                        val_loss, val_acc, val_preds, val_labels = self.validate(val_loader)\n",
    "\n",
    "                        #Upadate the learning rate scheduler\n",
    "                        self.scheduler.step(epoch + train_loss)\n",
    "\n",
    "                        #storing learning rate\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        self.history['learning_rates'].append(current_lr)\n",
    "\n",
    "                        #Save checkpoint\n",
    "                        self.save_checkpoint(epoch, best_acc, checkpoint_path)\n",
    "\n",
    "                        #Save the best model\n",
    "                        if val_acc > best_acc:\n",
    "                            best_acc = val_acc\n",
    "                            self.model_manager.save_model(best_model_path)\n",
    "                            print(f\"New best model saved with accuracy{best_acc:.f}%\")\n",
    "\n",
    "                        print(f'/nEpoch {epoch+1}/{self.config.num_epcohs}:')\n",
    "                        print(f'Train loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "                        print(f'Val loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n",
    "\n",
    "                        # Print learning rates for each parameter group\n",
    "                        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "                            if i == 0:\n",
    "                                print(f\"  Fine-tune LR: {param_group['lr']:.6f}\")\n",
    "                            else:\n",
    "                                print(f\"  New layers LR: {param_group['lr']:.6f}\")\n",
    "                        \n",
    "                        print(\"-\" * 60)\n",
    "                        # Load best model for final evaluation\n",
    "                    self.model_manager.load_model(best_model_path)\n",
    "                    print(f\"\\n✓ Fine-tuning completed for {self.model_manager.model_name}\")\n",
    "                    print(f\"✓ Best validation accuracy: {best_acc:.2f}%\")\n",
    "                    return self.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cd3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
